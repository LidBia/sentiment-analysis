{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nowedane.csv', encoding='latin-1')\n",
    "data = data.iloc[:, [0, -1]]\n",
    "data = data.rename(columns={data.columns[0]: 'rating', data.columns[-1]: 'text'})\n",
    "data.to_csv('nowedane.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_rating(rating):\n",
    "    if rating == 0:\n",
    "        return 'negative'\n",
    "    elif rating == 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('./nowedane.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the combined data into training, validation, and test sets\n",
    "train_set, temp_set = train_test_split(all_data, test_size=0.5, random_state=22)\n",
    "valid_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset the indices of the sets\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "valid_set.reset_index(drop=True, inplace=True)\n",
    "test_set.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the preprocess_rating function to the 'rating' column\n",
    "train_set['sentiment'] = train_set['rating'].apply(preprocess_rating)\n",
    "valid_set['sentiment'] = valid_set['rating'].apply(preprocess_rating)\n",
    "test_set['sentiment'] = test_set['rating'].apply(preprocess_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing 'text' values\n",
    "train_set = train_set.dropna(subset=['text'])\n",
    "valid_set = valid_set.dropna(subset=['text'])\n",
    "test_set = test_set.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the tokenizer and fit on the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_set['text'])\n",
    "\n",
    "# Determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_set['text'])\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_set['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_set['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_length = 400  # maximum sequence length\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4064s 162ms/step - loss: 0.4402 - accuracy: 0.7956 - val_loss: 0.4205 - val_accuracy: 0.8078\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(\n",
    "    train_set['sentiment'].map(sentiment_labels.index), num_classes=3)\n",
    "valid_labels = tf.keras.utils.to_categorical(\n",
    "    valid_set['sentiment'].map(sentiment_labels.index), num_classes=3)\n",
    "test_labels = tf.keras.utils.to_categorical(\n",
    "    test_set['sentiment'].map(sentiment_labels.index), num_classes=3)\n",
    "\n",
    "history = model.fit(train_padded, train_labels, epochs=1, validation_data=(valid_padded, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "training_accuracy = history.history['accuracy']\n",
    "validation_loss = history.history['val_loss']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "model.save('newmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence_padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(sequence_padded)\n",
    "    sentiment = sentiment_labels[prediction.argmax()]\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Shame on you\"\n",
    "classification = classify_sentiment(input_text)\n",
    "print(\"Sentiment:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
